# -*- coding: utf-8 -*-
"""heart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hKr22ohItfEGfgUMVmqrYZSvKWxOGqQO
"""

# import all usfull libraries.

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Import pandas library for data manipulation
import numpy as np # Import numpy library for numerical operations
import matplotlib.pyplot as plt # Import matplotlib for plotting
import seaborn as sns # Import seaborn for advanced plotting
import re # Import regular expression library for text manipulation
import string # Import string library for string operations
import nltk # Import nltk library for natural language processing
from nltk.corpus import stopwords # Import stopwords from nltk
from nltk.stem import PorterStemmer # Import PorterStemmer for stemming
from nltk.stem import WordNetLemmatizer # Import WordNetLemmatizer for lemmatizationimport re
import string
import nltk

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import nltk
import string
import re
from nltk.tokenize import word_tokenize
from scipy.sparse import hstack
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import math
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("/content/drive/MyDrive/all ml/heart_attack_risk_dataset.csv")

data.head()

data.shape

print(data['Age'].value_counts()) # Access the 'y' Series directly
plt.figure(figsize=(18, 6))
sns.countplot(x=data['Age'], data=data)

print(data.Age.value_counts())
plt.figure(figsize=(20, 6))
sns.countplot(y='Age', data=data)

data.info()

data.isna().sum()

#x=data.iloc[0:1].values
#y=data.iloc[:-1:].values

#x.shape

categorical_columns = data.select_dtypes(include=['object']).columns
label_encoder = LabelEncoder()

for column in categorical_columns:
    data[column] = label_encoder.fit_transform(data[column])


#label_encoder = LabelEncoder()
#data['gender']=le.fit_transform(data['gender'])
#data['Physical_Activity_Level']=le.fit_transform(data['Physical_Activity_Level'])
#data[' Stress_Level  ']=le.fit_transform(data[' Stress_Level  '])
#data['Chest_Pain_Type ']=le.fit_transform(data['Chest_Pain_Type '])
#data['Thalassemia']=le.fit_transform(data['Thalassemia'])
#data['ECG_Results ']=le.fit_transform(data['ECG_Results '])
#data['Heart_Attack_Risk ']=le.fit_transform(data['Heart_Attack_Risk '])

data.head()

balanced_data = pd.DataFrame()

# Iterate over each class
for class_label in data ['Heart_Attack_Risk'].unique():
    class_data =data [data['Heart_Attack_Risk'] == class_label]
    if len(class_data) >= 18000:
        sampled_data = class_data.sample(n=18000, random_state=42, replace=False)
    else:
       sampled_data = class_data.sample(n=18000, random_state=42, replace=True)

    balanced_data = pd.concat([balanced_data, sampled_data])

# Reset the index of the balanced dataset
balanced_data.reset_index(drop=True, inplace=True)

# Display the balanced dataset
print(balanced_data['Heart_Attack_Risk'].value_counts())

data=balanced_data

data.shape

y=data['Heart_Attack_Risk']
x=data.drop(columns=['Heart_Attack_Risk']) #independant  variable or input
#slipt
X_train,X_temp,y_train,y_temp=train_test_split(x, y, test_size=0.4, stratify=y, random_state=42)

X_test, X_cv, y_test, y_cv = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

print(y.value_counts()) # Access the 'y' Series directly
sns.countplot(x=y, data=data) # Pass the 'y' Series to the x argument of sns.countplot, and the DataFrame 'data' to the data argument

print(X_train.shape)
print(X_test.shape)
print(X_cv.shape)

#rmse

def rmse(actual, predicted) :
  return np.sqrt(((actual - predicted) ** 2).mean())

def rmse_to_prob(actual, predicted, max_rmse) :
  rmse_value = rmse(actual, predicted)
  probability = 1 - (rmse_value / max_rmse)

  return max(0, min(probability, 1))

"""# knn algorithm"""

# Train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)



# Predict on the test set and cross-validation set
y_test_pred = knn.predict(X_test)
y_cv_pred = knn.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set
cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Define the range of k values
k = list(range(1, 50, 2))

train_accuracy = []
cv_accuracy = []

# Loop over the range of k values to tune hyperparameter
for i in k:

    knn = KNeighborsClassifier(n_neighbors=i)

    # Train the classifier
    knn.fit(X_train, y_train)

    # Predict on train and CV sets
    y_train_pred= knn.predict(X_train)
    y_cv_pred = knn.predict(X_cv)

    # Calculate accuracy
    train_accuracy.append(accuracy_score(y_train, y_train_pred))
    cv_accuracy.append(accuracy_score(y_cv, y_cv_pred))


# Find the optimal k based on maximum CV accuracy
optimal_k = k[cv_accuracy.index(max(cv_accuracy))]


# Apply log transformation to k for plotting
k_log = [math.log(x) for x in k]
print(optimal_k)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(k_log, train_accuracy, label='Train Accuracy')
plt.plot(k_log, cv_accuracy, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (k)')
plt.xlabel('log(k)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Compute the confusion matrix for the test set
cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=optimal_k)
knn.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_train_pred =knn.predict(X_train)
y_test_pred = knn.predict(X_test)
y_cv_pred = knn.predict(X_cv)

# Evaluate the model
train_accuracy_knn= accuracy_score(y_train,y_train_pred)
test_accuracy_knn = accuracy_score(y_test, y_test_pred)
cv_accuracy_knn = accuracy_score(y_cv, y_cv_pred)

train_precision_knn= precision_score(y_train,y_train_pred, average='macro')
test_precision_knn = precision_score(y_test, y_test_pred, average='macro')
cv_precision_knn = precision_score(y_cv, y_cv_pred, average='macro')

train_recall_knn= recall_score(y_train,y_train_pred, average='macro')
test_recall_knn = recall_score(y_test, y_test_pred, average='macro')
cv_recall_knn = recall_score(y_cv, y_cv_pred, average='macro')

max_rmse = np.max(y_test)-np.min(y_test)
res =rmse_to_prob(y_test, y_test_pred, max_rmse)

print(train_accuracy_knn)
print(test_accuracy_knn)
print(cv_accuracy_knn)
print(train_precision_knn)
print(test_precision_knn)
print(cv_precision_knn)
print(train_recall_knn)
print(test_recall_knn)
print(cv_recall_knn)
print(max_rmse)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(k_log, train_accuracy, label='Train Accuracy')
plt.plot(k_log, cv_accuracy, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (k)')
plt.xlabel('log(k)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall','RMSE']
knn= pd.DataFrame([['KNN',train_accuracy_knn,test_accuracy_knn,cv_accuracy_knn,train_precision_knn,test_precision_knn,cv_precision_knn,train_recall_knn,test_recall_knn,cv_recall_knn,res]],columns=col)
#results.loc[1] = new

knn

"""# navie bayes"""

# Train the navie bayes
mnb = MultinomialNB()
mnb.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = mnb.predict(X_test)
y_cv_pred = mnb.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

alpha = [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in alpha:
    clf = MultinomialNB(alpha = i)
    clf.fit(X_train, y_train)

    y_train_pred = clf.predict(X_train)
    y_cv_pred = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_alpha= alpha[cv_auc.index(max(cv_auc))]
alpha=[math.log(x) for x in alpha]

print(optimal_alpha)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(alpha, train_auc, label='Train Accuracy')
plt.plot(alpha, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (alpha)')
plt.xlabel('log(alpha)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = MultinomialNB(alpha = optimal_alpha )
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred= clf.predict(X_cv)

# Evaluate the model
train_accuracy_nb= accuracy_score(y_train,y_train_pred)
test_accuracy_nb = accuracy_score(y_test, y_test_pred)
cv_accuracy_nb = accuracy_score(y_cv, y_cv_pred)

train_precision_nb= precision_score(y_train,y_train_pred, average='micro')
test_precision_nb = precision_score(y_test, y_test_pred, average='micro')
cv_precision_nb = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_nb= recall_score(y_train,y_train_pred, average='micro')
test_recall_nb = recall_score(y_test, y_test_pred, average='micro')
cv_recall_nb = recall_score(y_cv, y_cv_pred, average='micro')
max_rmse = np.max(y_test)-np.min(y_test)
res =rmse_to_prob(y_test, y_test_pred, max_rmse)

print(train_accuracy_nb)
print(test_accuracy_nb)
print(cv_accuracy_nb)
print(train_precision_nb)
print(test_precision_nb)
print(cv_precision_nb)
print(train_recall_nb)
print(test_recall_nb)
print(cv_recall_nb)
print(max_rmse)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall','RMSE']
nb = pd.DataFrame([['nb',train_accuracy_nb,test_accuracy_nb,cv_accuracy_nb,train_precision_nb,test_precision_nb,cv_precision_nb,train_recall_nb,test_recall_nb,cv_recall_nb,res]],columns=col)
#results.loc[1] = new

nb

"""


# Logistic RegressionL1
"""

# Train the Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report



log = LogisticRegression(penalty='l1',solver='liblinear')
log.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(X_test)
y_cv_pred = log.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    clf = LogisticRegression(penalty='l1',C=i,solver='liblinear')
    clf.fit(X_train, y_train)

    y_train_pred = clf.predict(X_train)
    y_cv_pred = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = LogisticRegression(penalty='l1',C = optimal_c, solver='liblinear')
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred_bow=clf.predict(X_train)
y_test_pred_bow = clf.predict(X_test)
y_cv_pred_bow = clf.predict(X_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred_bow)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']

logl1= pd.DataFrame([['log_l1',train_accuracy_log,  test_accuracy_log, cv_accuracy_log, train_precision_log, test_precision_log,  cv_precision_log, train_recall_log, test_recall_log,cv_recall_log]],columns=col)

logl1

"""# Logistic Regression L2"""

# Train the Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report



log = LogisticRegression(penalty='l2',solver='liblinear')
log.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(X_test)
y_cv_pred = log.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    clf = LogisticRegression(penalty='l2',C=i,solver='liblinear')
    clf.fit(X_train, y_train)

    y_train_pred = clf.predict(X_train)
    y_cv_pred = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = LogisticRegression(penalty='l2',C = optimal_c, solver='liblinear')
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred_bow)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']



logl2 = pd.DataFrame([['log_l2',train_accuracy_log,  test_accuracy_log, cv_accuracy_log, train_precision_log, test_precision_log,  cv_precision_log, train_recall_log, test_recall_log,cv_recall_log]],columns=col)

logl2

"""
# Logistic Regression with Elastic-Net (L1 and L2 combined)"""

# Train the Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report



log = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)
log.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(X_test)
y_cv_pred = log.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    clf = LogisticRegression(C=i,penalty='elasticnet', solver='saga', l1_ratio=0.5)
    clf.fit(X_train, y_train)

    y_train_pred_bow = clf.predict(X_train)
    y_cv_pred_bow = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = LogisticRegression(C = optimal_c,penalty='elasticnet', solver='saga', l1_ratio=0.5)
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
logl1andlogl2 = pd.DataFrame([['logl1andlogl2',train_accuracy_log,  test_accuracy_log, cv_accuracy_log, train_precision_log, test_precision_log,  cv_precision_log, train_recall_log, test_recall_log,cv_recall_log]],columns=col)

logl1andlogl2

result=pd.DataFrame()
result=pd.concat([result,logl1,logl2,logl1andlogl2,knn,nb],ignore_index=True)
result

"""# linear svm algorithm"""

from sklearn.linear_model import SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score
from sklearn.svm import LinearSVC



log = LinearSVC()
log.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(X_test)
y_cv_pred = log.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:

    clf = LinearSVC(C=i)
    clf.fit(X_train, y_train)

    y_train_pred_bow = clf.predict(X_train)
    y_cv_pred_bow = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = LinearSVC(C=optimal_c)
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
linearsvm = pd.DataFrame([['linearsvm',train_accuracy_log,  test_accuracy_log, cv_accuracy_log, train_precision_log, test_precision_log,  cv_precision_log, train_recall_log, test_recall_log,cv_recall_log]],columns=col)

linearsvm

"""# Applying RBF SVM"""

from sklearn.svm import SVC

rbf_svm = SVC(kernel='rbf')
log.fit(X_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(X_test)
y_cv_pred = log.predict(X_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:

    clf = SVC(C=i,kernel='rbf')
    clf.fit(X_train, y_train)

    y_train_pred_bow = clf.predict(X_train)
    y_cv_pred_bow = clf.predict(X_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = SVC(C=optimal_c,kernel='rbf')
clf.fit(X_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
RBFSVM = pd.DataFrame([['RBFSVM',train_accuracy_log,  test_accuracy_log, cv_accuracy_log, train_precision_log, test_precision_log,  cv_precision_log, train_recall_log, test_recall_log,cv_recall_log]],columns=col)

RBFSVM

result=pd.DataFrame()
result=pd.concat([result,knn,nb,logl1,logl2,logl1andlogl2,linearsvm,RBFSVM],ignore_index=True)
result

#https://heart-attack-analysis-prediction.streamlit.app/

"""# Applying Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV


dept = [1, 5, 10, 50, 100, 500, 1000]
min_samples =  [5, 10, 100, 500]


param_grid={'min_samples_split':min_samples , 'max_depth':dept}
clf = DecisionTreeClassifier()

model = GridSearchCV(clf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)
model.fit(X_train, y_train)

print("optimal min_samples_split",model.best_estimator_.min_samples_split)
print("optimal max_depth",model.best_estimator_.max_depth)

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot( train_auc, label='Train Accuracy')
plt.plot( cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

clf = DecisionTreeClassifier(max_depth = 1,min_samples_split = 5)
clf.fit(X_train,y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_dt= accuracy_score(y_train,y_train_pred)
test_accuracy_dt = accuracy_score(y_test, y_test_pred)
cv_accuracy_dt = accuracy_score(y_cv, y_cv_pred)

train_precision_dt= precision_score(y_train,y_train_pred, average='micro')
test_precision_dt = precision_score(y_test, y_test_pred, average='micro')
cv_precision_dt = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_dt= recall_score(y_train,y_train_pred, average='micro')
test_recall_dt = recall_score(y_test, y_test_pred, average='micro')
cv_recall_dt = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_dt)
print(test_accuracy_dt)
print(cv_accuracy_dt)
print(train_precision_dt)
print(test_precision_dt)
print(cv_precision_dt)
print(train_recall_dt)
print(test_recall_dt)
print(cv_recall_dt)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
Decisiontree = pd.DataFrame([['Decisiontree',train_accuracy_dt,  test_accuracy_dt, cv_accuracy_dt, train_precision_dt, test_precision_dt,  cv_precision_dt, train_recall_dt, test_recall_dt,cv_recall_dt]],columns=col)

result=pd.DataFrame()
result=pd.concat([result,knn,nb,logl1,logl2,logl1andlogl2,linearsvm,RBFSVM,Decisiontree],ignore_index=True)
result

"""# Random Forest"""

from sklearn.metrics import accuracy_score


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve

dept = [1, 5, 10, 50, 100, 500, 1000]
n_estimators =  [20, 40, 60, 80, 100, 120]

param_grid={'n_estimators':n_estimators , 'max_depth':dept}
clf = RandomForestClassifier()
model = GridSearchCV(clf,param_grid,scoring='accuracy',n_jobs=-1,cv=3)
model.fit(X_train, y_train)
print("optimal n_estimators",model.best_estimator_.n_estimators)
print("optimal max_depth",model.best_estimator_.max_depth)

optimal_max_depth = model.best_estimator_.max_depth
optimal_n_estimators = model.best_estimator_.n_estimators

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot( train_auc, label='Train Accuracy')
plt.plot( cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = RandomForestClassifier(max_depth = optimal_max_depth,n_estimators = optimal_n_estimators)
clf.fit(X_train, y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)
y_cv_pred = clf.predict(X_cv)

# Evaluate the model
train_accuracy_rd= accuracy_score(y_train,y_train_pred)
test_accuracy_rd = accuracy_score(y_test, y_test_pred)
cv_accuracy_rd = accuracy_score(y_cv, y_cv_pred)

train_precision_rd= precision_score(y_train,y_train_pred, average='micro')
test_precision_rd = precision_score(y_test, y_test_pred, average='micro')
cv_precision_rd = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_rd= recall_score(y_train,y_train_pred, average='micro')
test_recall_rd = recall_score(y_test, y_test_pred, average='micro')
cv_recall_rd = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_rd)
print(test_accuracy_rd)
print(cv_accuracy_rd)
print(train_precision_rd)
print(test_precision_rd)
print(cv_precision_rd)
print(train_recall_rd)
print(test_recall_rd)
print(cv_recall_rd)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
Randomforest = pd.DataFrame([['Randomforest',train_accuracy_rd,  test_accuracy_rd, cv_accuracy_rd, train_precision_rd, test_precision_rd,  cv_precision_rd, train_recall_rd, test_recall_rd,cv_recall_rd]],columns=col)

Randomforest

result=pd.DataFrame()
result=pd.concat([result,knn,nb,logl1,logl2,logl1andlogl2,linearsvm,RBFSVM,Decisiontree,Randomforest],ignore_index=True)
result

